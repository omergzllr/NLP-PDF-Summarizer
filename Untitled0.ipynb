{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPDmtbVZMqM/K/VsjxtiIKp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6G6H0jwieP2A","executionInfo":{"status":"ok","timestamp":1737573935778,"user_tz":-180,"elapsed":21382,"user":{"displayName":"omer güzeller","userId":"08681827711019106568"}},"outputId":"e46c90b0-30fb-454c-b6e3-f8bb502d34e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","# Google Drive'ı bağla\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Kütüphanelerin Kurulumu"],"metadata":{"id":"THgsJnRgejMF"}},{"cell_type":"code","source":["!pip install python-docx\n","!pip install spacy\n","!python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KqVyNiGoemES","executionInfo":{"status":"ok","timestamp":1737573984674,"user_tz":-180,"elapsed":21193,"user":{"displayName":"omer güzeller","userId":"08681827711019106568"}},"outputId":"f3f01608-d8a7-4bcf-828e-9edd0a4b82b2"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting python-docx\n","  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n","Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n","Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n","Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/244.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: python-docx\n","Successfully installed python-docx-1.1.2\n","Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting en-core-web-sm==3.7.1\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.10.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.27.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.8)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]}]},{"cell_type":"markdown","source":["#PROJE KODLARI"],"metadata":{"id":"aztezHV4gMCk"}},{"cell_type":"code","source":["import nltk\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.chunk import ne_chunk\n","from nltk.tag import pos_tag\n","import re\n","import docx\n","from collections import defaultdict\n","import spacy\n","import en_core_web_sm\n","import os\n","\n","\n","def download_nltk_resources():\n","    resources = [\n","        'punkt',\n","        'averaged_perceptron_tagger',\n","        'maxent_ne_chunker',\n","        'words',\n","        'stopwords',\n","        'punkt_tab'\n","    ]\n","    for resource in resources:\n","        try:\n","            nltk.download(resource, quiet=True)\n","        except Exception as e:\n","            print(f\"Warning: Could not download {resource}: {str(e)}\")\n","\n","\n","download_nltk_resources()\n","\n","class TranscriptTransformer:\n","    def __init__(self):\n","        # NTLK datalarının indirilmesi\n","        nltk.download('punkt')\n","        nltk.download('averaged_perceptron_tagger')\n","        nltk.download('maxent_ne_chunker')\n","        nltk.download('words')\n","        nltk.download('stopwords')\n","\n","        self.stop_words = set(stopwords.words('english'))\n","        self.nlp = spacy.load('en_core_web_sm')\n","\n","        # Anahtar kelimelerin tanımalanması\n","        self.key_concepts = {\n","            'SRE': ['site reliability engineering', 'reliability', 'reliable', 'sre team', 'site reliability'],\n","            'SLA_SLO': ['service level agreement', 'service level objective', 'sla', 'slo', 'availability', 'reliability target'],\n","            'ERROR_BUDGETS': ['error budget', 'budget', 'availability target', 'reliability target'],\n","            'DEV_OPS': ['development team', 'operations team', 'dev team', 'ops team', 'developers', 'operators'],\n","            'MONITORING': ['monitoring', 'alerts', 'metrics', 'measurement', 'tracking', 'observability'],\n","            'AUTOMATION': ['automate', 'automation', 'automated', 'script', 'tooling'],\n","            'POSTMORTEM': ['postmortem', 'post-mortem', 'incident review', 'blameless', 'root cause']\n","        }\n","\n","    def read_docx(self, file_path):\n","\n","        doc = docx.Document(file_path)\n","        text = []\n","        for paragraph in doc.paragraphs:\n","            text.append(paragraph.text)\n","        return '\\n'.join(text)\n","\n","    def preprocess_text(self, text):\n","\n","        # Cümle yapısını ve önemli noktalama işaretlerini koru\n","        text = re.sub(r'\\s+', ' ', text)\n","        text = re.sub(r'[^a-zA-Z0-9\\s.,!?()-:]', '', text)\n","        return text.strip()\n","\n","    def extract_key_topics(self, text):\n","\n","        doc = self.nlp(text.lower())\n","        topic_mentions = defaultdict(int)\n","\n","        # Anahtar kavramların geçme sıklığını say\n","        for concept, terms in self.key_concepts.items():\n","            for term in terms:\n","                topic_mentions[concept] += len(re.findall(r'\\b' + term + r'\\b', text.lower()))\n","\n","        # Konuları sıklığa göre sırala\n","        sorted_topics = sorted(topic_mentions.items(), key=lambda x: x[1], reverse=True)\n","        return [topic for topic, count in sorted_topics if count > 0]\n","\n","    def extract_learning_objectives(self, text):\n","\n","        objectives = [\n","            \"Understand the core principles of Site Reliability Engineering (SRE)\",\n","            \"Learn about SLA and SLO concepts and their importance\",\n","            \"Understand error budgets and their role in reliability\",\n","            \"Explore the relationship between Dev and Ops teams\",\n","            \"Learn about monitoring and automation practices\",\n","            \"Understand postmortem culture and blameless post-incident reviews\"\n","        ]\n","        return objectives\n","\n","    def extract_instructor_notes(self, section_text):\n","        #NLP kullanarak bölüm metninden eğitmen notları çıkar\n","        doc = self.nlp(section_text)\n","\n","        # Bölüm uzunluğu ve karmaşıklığına göre süre tahmin et\n","        word_count = len(section_text.split())\n","        time = '15-20 dakika' if word_count > 500 else '10-15 dakika'\n","\n","        # Cümle önemini kullanarak anahtar noktaları çıkar\n","        sentences = [sent.text.strip() for sent in doc.sents]\n","        key_points = []\n","\n","        for sent in sentences:\n","            if any(pattern in sent.lower() for pattern in [\n","                'is important', 'key', 'critical', 'essential', 'fundamental',\n","                'means that', 'refers to', 'is defined as', 'plays a role'\n","            ]):\n","                # Clean and shorten the sentence to make it concise\n","                point = re.sub(r'^.*?(is|means|refers)', '', sent).strip()\n","                point = re.sub(r'[.!?]$', '', point).strip()\n","                if len(point.split()) <= 10 and point not in key_points:\n","                    key_points.append(point)\n","\n","        # Yeterli anahtar nokta bulunamazsa\n","        if not key_points:\n","            key_points = [\n","                re.sub(r'[.!?]$', '', sent.strip())\n","                for sent in sentences[:3]\n","            ]\n","\n","        key_points = key_points[:3]  # En önemli 3 noktayı al\n","\n","        # Eylem kelimelerine göre aktiviteleri çıkar\n","        activities = []\n","        for sent in sentences:\n","            if any(word in sent.lower() for word in ['practice', 'exercise', 'analyze', 'review', 'discuss', 'examine']):\n","                activities.append(sent)\n","\n","        activity = activities[0] if activities else \"Group discussion on key concepts\"\n","\n","        # Soru kalıplarını kullanarak yaygın soruları oluştur\n","        doc_questions = [sent.text for sent in doc.sents if sent.text.strip().endswith('?')]\n","        common_questions = []\n","\n","        for question in doc_questions:\n","            # Konularla ilgili yaygın soruları filtrele\n","            if any(word in question.lower() for word in ['how', 'what', 'why', 'when', 'which']):\n","                if len(question.split()) <= 12:  # Soruları kısa tut\n","                    common_questions.append(question)\n","\n","        # Soru bulunamazsa\n","        if not common_questions:\n","            topic = section_text.split()[0] if section_text else \"this topic\"\n","            common_questions = [\n","                f\"How can we implement {topic} effectively?\",\n","                f\"What are the main challenges in {topic}?\"\n","            ]\n","\n","        common_questions = common_questions[:2]\n","\n","        return {\n","            'time': time,\n","            'key_points': key_points,\n","            'activities': activity,\n","            'common_questions': common_questions\n","        }\n","\n","    def add_instructor_notes(self, section_name, section_content):\n","\n","        notes = self.extract_instructor_notes(section_content)\n","\n","        return f\"\"\"\n","        INSTRUCTOR NOTES FOR {section_name.upper()}:\n","        ----------------------------------------\n","        Time Allocation: {notes['time']}\n","\n","        Key Points to Emphasize:\n","        {chr(10).join('- ' + point for point in notes['key_points'])}\n","\n","        Suggested Activities:\n","        {notes['activities']}\n","\n","        Common Questions to Prepare For:\n","        {chr(10).join('- ' + q for q in notes['common_questions'])}\n","        \"\"\"\n","\n","    def structure_content(self, text):\n","\n","        sentences = sent_tokenize(text)\n","        structured_content = defaultdict(list)\n","\n","        current_section = \"Introduction\"\n","\n","        # Bölüm anahtar kelimelerini tanımla\n","        section_keywords = {\n","            'Introduction': ['introduction', 'background', 'overview'],\n","            'SRE Fundamentals': ['reliability', 'engineering', 'fundamental'],\n","            'SLA and SLO': ['sla', 'slo', 'service level', 'availability'],\n","            'Error Budgets': ['error budget', 'reliability target'],\n","            'Dev and Ops Collaboration': ['dev team', 'ops team', 'collaboration'],\n","            'Monitoring and Automation': ['monitoring', 'automate', 'automation'],\n","            'Postmortem and Culture': ['postmortem', 'blameless', 'culture'],\n","            'Case Studies': ['example', 'case', 'instance'],\n","            'Best Practices': ['practice', 'recommendation', 'approach'],\n","            'Summary': ['conclusion', 'summary', 'finally']\n","        }\n","\n","        for sentence in sentences:\n","            sentence_lower = sentence.lower()\n","\n","            # Bölümü anahtar kelimelerle belirle\n","            for section, keywords in section_keywords.items():\n","                if any(keyword in sentence_lower for keyword in keywords):\n","                    current_section = section\n","                    break\n","\n","            structured_content[current_section].append(sentence)\n","\n","        # Eğitmen notlarının eklenmesi gereken bölümü güncelle\n","        for section, sentences in structured_content.items():\n","            section_text = \" \".join(sentences)\n","            structured_content[section] = {\n","                'content': sentences,\n","                'instructor_notes': self.extract_instructor_notes(section_text)\n","            }\n","\n","        return structured_content\n","\n","    def word_count_verification(self, text):\n","\n","        words = text.split()\n","        count = len(words)\n","        if count < 3900:\n","            print(f\"Warning: Text contains {count} words, minimum requirement is 3900 words\")\n","            return False\n","        print(f\"Word count requirement met: {count} words\")\n","        return True\n","\n","    def generate_section_summary(self, section_name, content):\n","\n","        # Eğer content bir liste ise, metne dönüştür\n","        if isinstance(content, list):\n","            text = ' '.join(content)\n","        else:\n","            text = content\n","\n","        doc = self.nlp(text)\n","\n","        # Önemli desenleri ve konumları kullanarak ana cümleleri çıkar\n","        important_sentences = []\n","\n","        # Önemli bilgiyi gösteren desenler\n","        importance_markers = [\n","            'key', 'important', 'essential', 'fundamental', 'primary',\n","            'focus', 'main', 'critical', 'crucial', 'significant',\n","            'introduces', 'explains', 'describes', 'covers', 'addresses'\n","        ]\n","\n","        # İlk cümleyi al, çoğunlukla ana konuyu içerir\n","        if len(list(doc.sents)) > 0:\n","            important_sentences.append(list(doc.sents)[0].text)\n","\n","\n","        for sent in doc.sents:\n","            sent_text = sent.text.lower()\n","            if any(marker in sent_text for marker in importance_markers):\n","                if sent.text not in important_sentences:\n","                    important_sentences.append(sent.text)\n","\n","\n","        topic_terms = []\n","        topic_name = section_name.lower()\n","\n","        for concept, terms in self.key_concepts.items():\n","            if any(term in topic_name for term in terms):\n","                topic_terms.extend(terms)\n","\n","        for sent in doc.sents:\n","            sent_text = sent.text.lower()\n","            term_count = sum(1 for term in topic_terms if term in sent_text)\n","            if term_count >= 2 and sent.text not in important_sentences:\n","                important_sentences.append(sent.text)\n","\n","\n","        important_sentences = important_sentences[:4]\n","\n","\n","        summary = f\"\"\"\n","        Summary: {important_sentences[0]}\n","        {' '.join(important_sentences[1:])}\n","        \"\"\"\n","\n","\n","        summary = re.sub(r'\\s+', ' ', summary)\n","        summary = summary.strip()\n","\n","        return summary\n","\n","    def generate_teaching_transcript(self, input_file):\n","\n","        raw_text = self.read_docx(input_file)\n","        cleaned_text = self.preprocess_text(raw_text)\n","\n","\n","        key_topics = self.extract_key_topics(cleaned_text)\n","        learning_objectives = self.extract_learning_objectives(cleaned_text)\n","        structured_content = self.structure_content(cleaned_text)\n","\n","\n","        output = []\n","\n","        #Başlık Oluuşturma\n","        output.append(\"SITE RELIABILITY ENGINEERING (SRE) - TEACHING GUIDE\\n\")\n","\n","        # Learning Objectives\n","        output.append(\"LEARNING OBJECTIVES:\")\n","        for i, objective in enumerate(learning_objectives, 1):\n","            output.append(f\"{i}. {objective}\")\n","        output.append(\"\\n\")\n","\n","        # Key Topics\n","        output.append(\"KEY TOPICS COVERED:\")\n","        for i, topic in enumerate(key_topics, 1):\n","            output.append(f\"{i}. {topic.replace('_', ' ').title()}\")\n","        output.append(\"\\n\")\n","\n","        for section, data in structured_content.items():\n","            output.append(f\"\\n{section.upper()}:\")\n","\n","\n","            output.append(self.add_instructor_notes(section, \" \".join(data['content'])))\n","\n","\n","            output.append(self.generate_section_summary(section, data['content']))\n","\n","\n","            output.append(\" \".join(data['content']))\n","            output.append(\"\\n\")\n","\n","        final_text = \"\\n\".join(output)\n","\n","        # Verify word count\n","        if not self.word_count_verification(final_text):\n","            print(\"Warning: Generated transcript may be too short for a 30-minute lecture\")\n","\n","        return final_text\n","\n","    def save_teaching_transcript(self, output_text, output_file):\n","\n","        doc = docx.Document()\n","\n","\n","        title = doc.add_heading('Site Reliability Engineering (SRE) - Teaching Guide', 0)\n","\n","\n","        sections = output_text.split('\\n\\n')\n","        for section in sections:\n","            if section.strip():\n","                if section.endswith(':'):\n","\n","                    doc.add_heading(section, level=1)\n","                else:\n","\n","                    para = doc.add_paragraph()\n","                    para.add_run(section)\n","\n","        doc.save(output_file)\n","\n","def main():\n","    transformer = TranscriptTransformer()\n","\n","\n","    input_file = \"/content/drive/MyDrive/Case_Study_Omer_Guzeller/metin.docx\"  #dosya yolu\n","    output_dir = \"/content/drive/MyDrive/Case_Study_Omer_Guzeller/output\"\n","    output_file = f\"{output_dir}/egitim_materyali_transformu.docx\"\n","\n","\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    if not os.path.exists(input_file):\n","        print(f\"Error: Input file '{input_file}' not found at {os.path.abspath(input_file)}\")\n","        return\n","\n","    try:\n","        raw_text = transformer.read_docx(input_file)\n","        if not raw_text.strip():\n","            print(\"Error: Input file is empty or could not be read properly\")\n","            return\n","\n","        teaching_transcript = transformer.generate_teaching_transcript(input_file)\n","        transformer.save_teaching_transcript(teaching_transcript, output_file)\n","        print(f\"Teaching transcript successfully generated and saved to {output_file}\")\n","\n","        word_count = len(teaching_transcript.split())\n","        print(f\"Generated transcript contains {word_count} words\")\n","\n","    except Exception as e:\n","        print(f\"Error processing transcript: {str(e)}\")\n","        print(f\"Current working directory: {os.getcwd()}\")\n","        print(f\"Input file absolute path: {os.path.abspath(input_file)}\")\n","        print(\"Please ensure the input file is a valid DOCX file and has readable content.\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ddu_di-FgNTZ","executionInfo":{"status":"ok","timestamp":1737575906788,"user_tz":-180,"elapsed":7501,"user":{"displayName":"omer güzeller","userId":"08681827711019106568"}},"outputId":"534a0bbf-53f2-47a1-a2f2-111bc7c111dc"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Word count requirement met: 8167 words\n","Teaching transcript successfully generated and saved to /content/drive/MyDrive/Case_Study_Omer_Guzeller/output/egitim_materyali_transformu.docx\n","Generated transcript contains 8167 words\n"]}]}]}